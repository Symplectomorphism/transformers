# Aykut's Transformers
I'm working on various transformer architectures here.

## Done
1. Vanilla transformer implemented based on [Umar Jamil's lecture](https://youtu.be/ISNdQcPhsts?si=kd00dN1GiQRQgQoj), [sample code](https://github.com/hkproj/pytorch-transformer).
2. GPT implemented based on [Andrej Karpathy's lecture](https://youtu.be/kCc8FmEb1nY?si=ZnAfFHPO1ly3Ie0t), [sample code](https://github.com/karpathy/ng-video-lecture).

## In Progress
1. Decision transformer on the shortest route problem: [paper](https://arxiv.org/pdf/2106.01345), [sample code](https://github.com/kzl/decision-transformer).

## To be done
1. Vision Transformer
2. Transformer with [MoE Layer](https://arxiv.org/pdf/1701.06538), [sample code](https://github.com/YeonwooSung/Pytorch_mixture-of-experts?tab=readme-ov-file).  
   a. Nice pictures [here](https://www.linkedin.com/pulse/mixture-experts-moe-transformers-impact-large-language-nikhil-goel-fj2tc/).  
   b. Another [implementation](https://gist.github.com/ruvnet/0928768dd1e4af8816e31dde0a0205d5).
3. Transformer with [Rotary Position Embedding](https://arxiv.org/pdf/2104.09864), [sample code](https://github.com/ZhuiyiTechnology/roformer).
4. Transformer with [KAN layers](https://arxiv.org/pdf/2404.19756).
