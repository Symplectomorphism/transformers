# Aykut's Transformers
I'm working on various transformer architectures here.

## Done
1. Vanilla transformer implemented based on [Umar Jamil's lecture](https://youtu.be/ISNdQcPhsts?si=kd00dN1GiQRQgQoj), [sample code](https://github.com/hkproj/pytorch-transformer).
2. GPT implemented based on [Andrej Karpathy's lecture](https://youtu.be/kCc8FmEb1nY?si=ZnAfFHPO1ly3Ie0t), [sample code](https://github.com/karpathy/ng-video-lecture).

## In Progress
1. Decision transformer on the shortest route problem: [paper](https://arxiv.org/pdf/2106.01345), [sample code](https://github.com/kzl/decision-transformer).

## To be done
1. Transformer with [MoE Layer](https://arxiv.org/pdf/1701.06538), [sample code](https://github.com/YeonwooSung/Pytorch_mixture-of-experts?tab=readme-ov-file).
2. Transformer with [Rotary Position Embedding](https://arxiv.org/pdf/2104.09864), [sample code](https://github.com/ZhuiyiTechnology/roformer).
3. Transformer with [KAN layers](https://arxiv.org/pdf/2404.19756).
