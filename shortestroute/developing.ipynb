{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.utils import set_seed\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
    "import time\n",
    "\n",
    "import routegym.env\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity = 0.56\n",
      "[[0 0 1 1 0 0 0 0 1 0 0 1]\n",
      " [0 0 1 0 0 1 1 0 1 1 0 1]\n",
      " [1 1 0 0 1 1 1 1 0 0 0 1]\n",
      " [1 1 0 0 0 1 0 0 1 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0 1 1 0 1 1]\n",
      " [1 0 0 0 0 1 0 1 0 0 1 0]\n",
      " [0 0 0 0 1 1 0 0 1 0 0 1]\n",
      " [0 0 0 0 1 0 0 1 0 1 1 1]\n",
      " [0 0 0 1 0 1 1 1 1 0 1 1]\n",
      " [1 0 1 0 0 0 1 0 1 1 0 1]\n",
      " [0 0 0 0 0 1 1 1 1 1 0 0]]\n",
      "[[-1 -1  1  1 -1  1  1 -1  1 -1  1  1]\n",
      " [-1 -1  1  1  1  1  1 -1  1  1 -1  1]\n",
      " [ 1  1 -1 -1  1  1  1  1 -1 -1  1  1]\n",
      " [ 1  1 -1 -1 -1  1 -1 -1  1  1 -1 -1]\n",
      " [-1  1  1 -1 -1  1  1  1  1 -1 -1  1]\n",
      " [ 1  1  1  1  1 -1  1  1  1  1  1  1]\n",
      " [ 1  1  1 -1  1  1 -1  1 -1  1  1  1]\n",
      " [-1 -1  1 -1  1  1  1 -1  1  1 -1  1]\n",
      " [ 1  1 -1  1  1  1 -1  1 -1  1  1  1]\n",
      " [-1  1 -1  1 -1  1  1  1  1 -1  1  1]\n",
      " [ 1 -1  1 -1 -1  1  1 -1  1  1 -1  1]\n",
      " [ 1  1  1 -1  1  1  1  1  1  1  1 -1]]\n"
     ]
    }
   ],
   "source": [
    "node_number = 12\n",
    "\n",
    "env = None\n",
    "while env is None:\n",
    "    try:\n",
    "        A = sp.sparse.random(node_number, node_number, density=0.5, format='csr')\n",
    "        A.data[:] = 1\n",
    "        A = A.todense()\n",
    "        A = np.ma.array(A, mask=np.eye(node_number)).filled(fill_value=0).astype(int)\n",
    "        print(\"sparsity = %.2f\" % (1 - np.sum(A)/A.size))\n",
    "        G = nx.from_numpy_array(A)\n",
    "        env = routegym.env.ShortestRouteEnv(G, 0, 5, random_weights=(1,10))\n",
    "    except:\n",
    "        pass\n",
    "# env.render()\n",
    "print(A)\n",
    "print(env.graph.adj_mat)\n",
    "# print(env.get_dijkstra())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(30):\n",
    "    env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position: {0, 2, 10, 10, 10, 10, 10, 2, 1, 11, 9, 3, 3, 3, 3, 3, 3, 5, }\n",
      "\n",
      "Final reward: -17.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rew = 0\n",
    "position = 0\n",
    "env.reset()\n",
    "done = False\n",
    "print(\"Position: {\", end='')\n",
    "while not done:\n",
    "    action = np.random.choice(np.arange(0, env.graph.adj_mat.shape[0]))\n",
    "    position, reward, done, _ = env.step(action)\n",
    "    # env.render()\n",
    "    print(\"%d, \" % position, end='')\n",
    "    rew += reward\n",
    "print(\"}\\n\")\n",
    "print(\"Final reward: %.2f\" % rew)\n",
    "env.reset()\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "num_steps = 10_000\n",
    "\n",
    "def create_dataset():\n",
    "    obss = []\n",
    "    actions = []\n",
    "    returns = [0]\n",
    "    done_idxs = []\n",
    "    stepwise_returns = []\n",
    "\n",
    "    # simulate to create trajectories\n",
    "    transitions_per_buffer = np.zeros(50, dtype=int)\n",
    "    num_trajectories = 0\n",
    "    while len(obss) < num_steps:\n",
    "        done = False\n",
    "        for _ in range(10):\n",
    "            ac = np.random.choice(np.arange(0, env.graph.adj_mat.shape[0]))\n",
    "            state, reward, done, _ = env.step(ac)\n",
    "            obss += [state]\n",
    "            actions += [ac]\n",
    "            stepwise_returns += [reward]\n",
    "            returns[-1] += reward\n",
    "        # done = False\n",
    "        env.reset()\n",
    "        done_idxs += [len(obss)]\n",
    "        returns += [0]\n",
    "\n",
    "    actions = np.array(actions)\n",
    "    returns = np.array(returns)\n",
    "    stepwise_returns = np.array(stepwise_returns)\n",
    "    done_idxs = np.array(done_idxs)\n",
    "\n",
    "    # create reward-to-go dataset\n",
    "    start_index = 0\n",
    "    rtg = np.zeros_like(stepwise_returns)\n",
    "    for i in done_idxs:\n",
    "        i = int(i)\n",
    "        curr_traj_returns = stepwise_returns[start_index:i]\n",
    "        for j in range(i-1, start_index-1, -1): # start from i-1\n",
    "            rtg_j = curr_traj_returns[j-start_index:i-start_index]\n",
    "            rtg[j] = sum(rtg_j)\n",
    "        start_index = i\n",
    "    print('max rtg is %d' % max(rtg))\n",
    "\n",
    "    # create timestep dataset\n",
    "    start_index = 0\n",
    "    timesteps = np.zeros(len(actions)+1, dtype=int)\n",
    "    for i in done_idxs:\n",
    "        i = int(i)\n",
    "        timesteps[start_index:i+1] = np.arange(i+1 - start_index)\n",
    "        start_index = i+1\n",
    "    print('max timesteps is %d' % max(timesteps))\n",
    "\n",
    "    return obss, actions, returns, done_idxs, rtg, timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max rtg is 0\n",
      "max timesteps is 10\n",
      "-6.0\n"
     ]
    }
   ],
   "source": [
    "obss, actions, returns, done_idxs, rtgs, timesteps = create_dataset()\n",
    "print(returns[:-1].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateActionReturnDataset(Dataset):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CN()\n",
    "        C.block_size = 10 * 3\n",
    "        return C\n",
    "\n",
    "    def __init__(self, data, block_size, actions, done_idxs, rtgs, timesteps):\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = max(actions) + 1\n",
    "        self.data = data\n",
    "        self.actions = actions\n",
    "        self.done_idxs = done_idxs\n",
    "        self.rtgs = rtgs\n",
    "        self.timesteps = timesteps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        block_size = self.block_size // 3\n",
    "        done_idx = idx + block_size\n",
    "        for i in self.done_idxs:\n",
    "            if i > idx: # first done_idx is greater than idx\n",
    "                done_idx = min(int(i), done_idx)\n",
    "                break\n",
    "        idx = done_idx - block_size\n",
    "        states = torch.tensor(np.array(self.data[idx:done_idx]), dtype=torch.float32).reshape(block_size, -1) # (block_size, state_dim)\n",
    "        actions = torch.tensor(self.actions[idx:done_idx], dtype=torch.long).unsqueeze(1) # (block_size, 1)\n",
    "        rtgs = torch.tensor(self.rtgs[idx:done_idx], dtype=torch.float32).unsqueeze(1) # (block_size, 1)\n",
    "        timesteps = torch.tensor(self.timesteps[idx:idx+1], dtype=torch.int64).unsqueeze(1) # (block_size, 1)\n",
    "\n",
    "        return states, actions, rtgs, timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = CN()\n",
    "\n",
    "# system\n",
    "C.system = CN()\n",
    "C.system.seed = 3407\n",
    "C.system.work_dir = './out/decgpt'\n",
    "\n",
    "# data\n",
    "C.data = StateActionReturnDataset.get_default_config()\n",
    "\n",
    "# model \n",
    "C.model = GPT.get_default_config()\n",
    "C.model.model_type = 'gpt-mini'\n",
    "\n",
    "# trainer\n",
    "C.trainer = Trainer.get_default_config()\n",
    "C.trainer.learning_rate = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mingpt.utils.CfgNode at 0x735a5075d4d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = StateActionReturnDataset(obss, 10 * 3, actions, done_idxs, rtgs, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dec-transformer-route",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
